{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from pymilvus import MilvusClient\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_FILE = \"WinRAR.pdf\"\n",
    "DB_FILE_PATH = \"milvus_db.db\"\n",
    "COLLECTION_NAME = \"WinRAR\"\n",
    "\n",
    "EMBED_MODEL_NAME = \"intfloat/e5-large-v2\"\n",
    "EMBED_DIM = 1024\n",
    "\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "prompt_template = (\n",
    "    \"You are a helpful assistant.\\n\"\n",
    "    \"Based on the question from the user, I have prepared some context that may be related to the question, which is given below:\\n\"\n",
    "    \"{}\\n\"\n",
    "    \"\\n\"\n",
    "    \"And here is the question: {}\\n\"\n",
    "    \"\\n\"\n",
    "    \"Please provide a useful answer.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_client = MilvusClient(uri=DB_FILE_PATH)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    dimension=EMBED_DIM,\n",
    "    metric_type=\"L2\",\n",
    "    consistency_level=\"Strong\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading PDF and splitting texts...\")\n",
    "loader = PyPDFLoader(PDF_FILE)\n",
    "docs = loader.load_and_split()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "print(\n",
    "    f\"Total splitted docs have {len(split_docs)} chunks, first chunk is:\\n{split_docs[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "data_batch = []\n",
    "for i, d in enumerate(split_docs):\n",
    "    vec = embeddings.embed_query(d.page_content)\n",
    "    data_batch.append({\n",
    "        \"id\": i,\n",
    "        \"vector\": vec,\n",
    "        \"text\": d.page_content.replace(\"\\n\", \" \")\n",
    "    })\n",
    "\n",
    "print(\"Vectors ready...\")\n",
    "\n",
    "insert_res = milvus_client.insert(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    data=data_batch\n",
    ")\n",
    "\n",
    "print(f\"Insert done. Total inserted: {len(data_batch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def milvus_search(query: str, top_k: int = 3):\n",
    "    # Query from Milvus\n",
    "    query_vec = embeddings.embed_query(query)\n",
    "    results = milvus_client.search(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        data=[query_vec],\n",
    "        limit=top_k,\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    hits = results[0]\n",
    "\n",
    "    # Extract the text and score from the result\n",
    "    docs_found = []\n",
    "    for h in hits:\n",
    "        doc_text = h['entity']['text']\n",
    "        doc_score = h['distance']\n",
    "        docs_found.append({\n",
    "            \"text\": doc_text,\n",
    "            \"score\": doc_score\n",
    "        })\n",
    "    return docs_found\n",
    "\n",
    "\n",
    "def generate_answer_with_gpt(query: str, context_list: List[str]) -> str:\n",
    "    context_str = \"\\n\".join(f\"- {c}\" for c in context_list)\n",
    "    prompt = prompt_template.format(context_str, query)\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    ans = response.choices[0].message.content\n",
    "    return prompt, ans.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"What limitations does WinRAR have?\"\n",
    "print(f\"\\n[User Question] {user_question}\")\n",
    "\n",
    "print(\"------ Searching Milvus... ------\")\n",
    "top_docs = milvus_search(user_question, top_k=5)\n",
    "for i, d in enumerate(top_docs):\n",
    "    print(f\"Doc {i+1} - Score {d['score']} : {d['text']}\")\n",
    "\n",
    "context_texts = [d[\"text\"] for d in top_docs]\n",
    "\n",
    "print(\"------ Generating answer with GPT... ------\")\n",
    "prompt, answer = generate_answer_with_gpt(user_question, context_texts)\n",
    "print(\"\\n===== GPT Prompt =====\")\n",
    "print(prompt)\n",
    "print(\"\\n===== RAG Answer =====\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
